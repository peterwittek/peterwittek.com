Title: Advances in quantum machine learning in 2016 and in early 2017
Date: 2017-05-13 21:05
Author: Peter
Category: Quantum machine learning
Tags: Quantum machine learning, Machine learning, Quantum information theory
Slug: qml-2016-17
Summary: Desperately trying to keep up with the latest developments in quantum machine learning, let that be a new quantum-enhanced learning protocol, or some exciting connection between quantum many-body physics and statistical learning theory

It has been nearly a year and a half since I [last tried]({filename}/qml-in-2015.md) to digest the flood of QML papers. My backlog exceeded a hundred unread papers again, so I cannot postpone processing them anymore. I give up any attempt at being comprehensive -- it would be futile.

The terminology is evolving. We used to mean quantum-enhanced ML by saying QML, that is, learning algorithms that use a quantum protocol for an improvement. The meaning of QML is expanding: it includes quantum-enhanced learning algorithms, but also a new application area of learning theory in many-body physics, and now we can also safely lump a whole range of applications of classical ML in quantum physics under this moniker. Perhaps the most accurate definition of QML is that as long as the work has an AI/ML-ish element plus something vaguely quantum, it belongs here. I am perfectly content with this definition, and I reviewed the latest advances with this broad view on the topic. Below I summarize my much biased findings.

No shortage of events
---------------------
There have been at least three workshops: [one in Belgium](https://www.elen.ucl.ac.be/esann/index.php?pg=esann16_programme), [one in South Africa](http://www.quantummachinelearning.org/qml-workshop-2016.html), and [one in Canada](https://perimeterinstitute.ca/conferences/quantum-machine-learning). A workshop on machine learning and quanym many-body physics is [coming up soon](http://kits.ucas.ac.cn/index.php/events/workshop/52-machine-learning-and-many-body-physics-jun-28th-jul-7th-2017) in China. South Africa also hosted a [summer school](http://www.quantummachinelearning.org/qml-summer-school-2017.html), which was the best scientific event I ever showed up on. On regular conferences, NIPS had at least two QML-related papers, although we failed with our application to follow up the first QML workshop there.

The topic is slowly moving out of purely academic interest. Toronto has a relevant [Meetup group](https://www.meetup.com/Quantum-Computing-and-Big-Data/). The Creative Destruction Lab in Toronto is busy organizing the first cohort of a [startup incubator](https://www.creativedestructionlab.com/quantum/) focusing on QML technologies -- I happen to be involved with this one. I am also organizing a [reading group](https://github.com/peterwittek/qml-rg) in my institute, which is sufficiently open to attract the occasional third-party contribution. The [quantummachinelearning.org](http://quantummachinelearning.org/) was revised to act as a gateway to information. The revision was so successful that now we exceed a hundred visitors per month.
We also set up a [LinkedIn group](https://www.linkedin.com/groups/8592758) that does not do much yet.

No shortage of reviews
----------------------
Apart from a book, and introduction and an overview paper from earlier, now we have a fairly comprehensive laundry list of QML papers [@biamonte2016quantum], and a survey on the theoretical foundations of quantum learning [@arunachalam2017survey]. Add to this a [revised Wikipedia article](https://en.wikipedia.org/wiki/Quantum_machine_learning), and another introduction [@schuld2016quantumencyclopedia], and you will realize you are spoilt for choices if you want to get your head around.

Machine learning and quantum many-body physics
----------------------------------------------
Easily the most interesting development is that the prehistoric connection between many-body physics and machine learning has been revived, producing a series of high-profile papers. Most notably, representing quantum states by Boltzmann machines lead to a Science paper [@carleo2016solving] Later it was shown that the ansatz holds well for long-range entanglement, and given how compact this representation is, the implications are far reaching [@deng2016exact] [@deng2017quantum]. Another study showed a form of no-flattening theorem, showing that non-deep Boltzmann machines have their limits [@gao2017efficient].

A parallel line of work studied classical [@carrasquilla2016machine] and quantum phase transitions [@broecker2016machine] in a supervised learning scenario. This line of thought was extended to unsupervised settings, not knowing the transition point *a priori* [@nieuwenburg2017learning] [@wetzel2017unsupervised]. Boltzmann machines come handy for many-body state tomography [@kieferova2016tomography] [@torlai2017many]. A marginally related new application is to classify separable and entangled states: one approach is via old-school feature engineering of the state space and then bagging[@ma2017transforming], and another one is via shallow neural networks deployed on correlations [@lu2017separability]. Applying machine learning to tackle fundamental questions in physics [is becoming a standard tool](https://www.nature.com/nphys/journal/v13/n5/full/nphys4053.html).

If states can be represented by learning methods, and learning algorithms are so successful in a range of physics problems, we may wonder if there is a more profound connection. Initial signs indicate an affirmative answer. Tensor networks and deep learning [have some natural similarities](https://medium.com/intuitionmachine/the-holographic-principle-and-deep-learning-52c2d6da8d9). We can map restricted Boltzmann machines to tensor network states [@chen2017equivelence], and we can establish an equivalence between convolutional arithmetic networks and tensor networks [@cohen2017analysis].

More annealing
--------------
We can never get enough of quantum annealing. Articles on the topic keep multiplying great numbers, but I perceive a clear trend emerging, at least as far as D-Wave's commercial architecture is concerned. The attention shifted from finding the optimum of discrete optimization problems to sampling. First of all, simulated quantum annealing can be exponentially faster than ordinary simulated annealing [@crosson2016simulated], and the result on achieving $10^8$-times speedup on a hand-crafted problem by quantum annealing is being eroded in more realistic scenarios $10^8$-times speedup [@mandra2016strengths] [@king2017quantum]. A negative result on boosting also emerged [@dulny2016developing]. So the paradigm shift is welcome.

On the sampling side, much have been happening. First of all, we can train a truly quantum Boltzmann machine by adding a transverse term in the Hamiltonian, although the impact on learning is inconclusive [@amin2016quantum]. It already has an extension to reinforcement learning [@crawford2016reinforcement]. Effective temperature estimation remains a problem [@raymond2016global], but we can train fully visible Boltzmann machines [@korenkevych2016benchmarking], and, in fact, arbitrarily connected probabilistic graphical models [@benedetti2016quantum]

QMLN paper

A twist on the learning by annealing paradigm is learn to anneal, which is the reverse problem: we give a target state we want at the end of annealing, and we learn the weights of the spin system [@behrman2016learning].

Finally of pure theoretical interest: we should consider open quantum systems in the adiabatic framework [@wild2016adiabatic]. I wonder why there aren't more studies on machine learning and open quantum systems.

Quantum neural networks that make sense
---------------------------------------
I am negatively biased against any paper that discusses quantum neural networks, but a few papers passed my filter recently. A [cool blog post](https://silky.github.io/posts/2016-12-11-quantum-neural-networks.html) gives more details. A highlight is a quantum photonic neural network with applications in physics, where the backpropagation phase is fully classical, and the number of ancilla system might blow up [@wan2016quantum]. Similar applications show up in [@romero2016quantum].

[@wiebe2016quantumperceptron] quantum perceptron by Grover's, rigorous bounds

[@daskin2016quantumimplementation] phase estimation for Widrow-Hoff formula,

Each to his own [@potok2017study]

HHL plateau
-----------
We are really running out of options where we can deploy the [HHL algorithm](https://en.wikipedia.org/wiki/Quantum_algorithm_for_linear_systems_of_equations) for an exponential boost. We have seen a quantum singular value decomposition protocol of nonsparse low-rank matrices[@rebentrost2016quantumsingular], quantum discrimant analysis [@cong2016quantum], and data fitting for prediction [@schuld2016prediction]. 

[@lau2016quantum] continuous-variable systems

[@rebentrost2016quantum]

Reinforcement learning
----------------------
[@august2016using]

[@mavadia2016prediction] predict qubit decoherence

[@lamata2017basic] elementary reinforcement learning on superconducting qubits

[@clausen2016quantum] stepwise modifications of quantum channel properties

[@naruse2016single] not exactly reinforcement learning, but a photonic system for decision problems

Quantum control with GPU-accelerated autograd [@leung2016speedup]. Landscape is almost always trap-free [@russell2016quantum]

Quantum machine learning at your fingertips
-------------------------------------------
[@schuld2017quantum] IBMQE for distances

[@chatterjee2016generalized] coherent states to calculate RBF kernel, generalizations thereof, quantum optical system

[@lamata2017basic]

[@lau2016quantum] continuous-variable systems

Quantum learning theory
-----------------------
[@holik2016pattern] non-Kolmogorivan pattern recognition

[@grilo2017learning] establishes a connection between quantum learning and cryptography

Arunachalam papers

Monras
